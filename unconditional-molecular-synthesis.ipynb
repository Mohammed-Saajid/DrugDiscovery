{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12906801,"sourceType":"datasetVersion","datasetId":8166688}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unconditional Molecular Generation\n\n**Definition**: Molecule Generation is to generate diverse, novel molecules that has desirable chemical properties. These properties are measured by oracle functions. A machine learning task first learns the molecular characteristics from a large set of molecules where each is evaluated through the oracles. Then, from the learned distribution, we can obtain novel candidates.\n\n**Impact**: As the entire chemical space is far too large to screen for each target, high through screening can only be restricted to a set of existing molecule libraries. Many novel drug candidates are thus usually omitted. Machine learning that can generate novel molecules obeying some pre-defined optimal properties can circumvent this problem and obtain novel classes of candidates.\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-04-16T05:46:16.142618Z","iopub.status.busy":"2025-04-16T05:46:16.142220Z","iopub.status.idle":"2025-04-16T05:46:38.311669Z","shell.execute_reply":"2025-04-16T05:46:38.310963Z","shell.execute_reply.started":"2025-04-16T05:46:16.142597Z"}}},{"cell_type":"markdown","source":"## Install Dependencies\nTo run the project smoothly, you’ll need to install several Python libraries. Below is a list of required dependencies along with their purpose:\n\n- **TensorFlow**: A popular deep learning framework used for building and training neural networks.\n- **RDKit**: A collection of cheminformatics and machine learning tools.\n- **Scikit-learn**: A library for machine learning in Python, providing simple and efficient tools for data mining and data analysis.\n- **FCD-Torch**: A library for fast computation of molecular fingerprints.\n- **numpy**: A fundamental package for scientific computing in Python, providing support for arrays, matrices, and a wide range of mathematical functions.\n- **pandas**: A library for data manipulation and analysis, providing data structures like DataFrames.","metadata":{}},{"cell_type":"code","source":"! pip install selfies pympler  -q ","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:10.444124Z","iopub.execute_input":"2025-08-29T14:24:10.444475Z","iopub.status.idle":"2025-08-29T14:24:13.411949Z","shell.execute_reply.started":"2025-08-29T14:24:10.444452Z","shell.execute_reply":"2025-08-29T14:24:13.410955Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Import Necessary libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport selfies as sf\nimport pandas as pd \nimport numpy as np \nimport random\nimport keras\nimport math\nimport os\nimport re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Embedding, MultiHeadAttention, LayerNormalization, Dropout, Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:13.413503Z","iopub.execute_input":"2025-08-29T14:24:13.413799Z","iopub.status.idle":"2025-08-29T14:24:13.610514Z","shell.execute_reply.started":"2025-08-29T14:24:13.413767Z","shell.execute_reply":"2025-08-29T14:24:13.609942Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Define metrics, optimizer, Global Variables","metadata":{}},{"cell_type":"code","source":"# Model parameters\nBATCH_SIZE = 1024\nEMBEDDING_DIM = 256\nSEQ_LENGTH = 50\nNUM_HEADS = 8\nDFF = 1024\n# Other variables\nDATASET_PATH = \"/kaggle/input/drug-discovery/data.csv\"\nLR = 1e-4\n\nmetrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\noptimizer = tf.keras.optimizers.Adam(learning_rate=LR)","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:13.611141Z","iopub.execute_input":"2025-08-29T14:24:13.611353Z","iopub.status.idle":"2025-08-29T14:24:15.352137Z","shell.execute_reply.started":"2025-08-29T14:24:13.611337Z","shell.execute_reply":"2025-08-29T14:24:15.351511Z"},"trusted":true},"outputs":[{"name":"stderr","text":"I0000 00:00:1756477455.286006      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Set seeds for reproducibility","metadata":{}},{"cell_type":"code","source":"# Set seeds\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T14:24:15.353797Z","iopub.execute_input":"2025-08-29T14:24:15.354061Z","iopub.status.idle":"2025-08-29T14:24:15.357974Z","shell.execute_reply.started":"2025-08-29T14:24:15.354044Z","shell.execute_reply":"2025-08-29T14:24:15.357228Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Defining Custom Masked Loss Function","metadata":{}},{"cell_type":"code","source":"@keras.saving.register_keras_serializable()\ndef masked_loss(y_true, y_pred):\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True,\n        reduction='none'\n    )\n    loss = loss_fn(y_true, y_pred)\n    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n    loss = loss * mask\n    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:15.358687Z","iopub.execute_input":"2025-08-29T14:24:15.358924Z","iopub.status.idle":"2025-08-29T14:24:15.373729Z","shell.execute_reply.started":"2025-08-29T14:24:15.358903Z","shell.execute_reply":"2025-08-29T14:24:15.373133Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Defining Model Architecture\n","metadata":{}},{"cell_type":"code","source":"@keras.saving.register_keras_serializable()\nclass LongTermMemory(tf.keras.layers.Layer):\n    \"\"\"\n    Long-term memory layer that compresses and refines input representations.\n    \"\"\"\n    def __init__(self, units, activation=tf.keras.activations.silu, **kwargs):\n        super(LongTermMemory, self).__init__(**kwargs)\n        \"\"\"\n        A long-term memory layer that compresses and refines input representations.\n        Args:\n            units (int): The number of output units.\n            activation (callable): The activation function to use.\n        Returns:\n            None\n        \"\"\"\n        super(LongTermMemory, self).__init__()\n        self.units = units\n        self.activation = activation\n        self.name = \"Long_Term_Memory\"\n\n        # Define layers\n        self.fc1 = Dense(self.units, activation=self.activation)\n        self.fc2 = Dense(self.units * 2, activation=self.activation)\n        self.fc3 = Dense(self.units, activation=self.activation)\n    \n    def call(self, inputs, mask=None):\n        x = self.fc1(inputs)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n\n@keras.saving.register_keras_serializable()\nclass Memory(tf.keras.layers.Layer):\n    \"\"\"\n    Memory module with long-term + persistent memory integration.\n    \"\"\"\n    def __init__(self, embedding_dim, sequence_length, activation=tf.keras.activations.silu, **kwargs):\n        super(Memory, self).__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.sequence_length = sequence_length\n        self.activation = activation\n        self.name = \"Memory\"\n\n    \n        # Query transformation\n        self.LMWq = Dense(units=self.embedding_dim, activation=self.activation, use_bias=False, name=\"Query_Transformation_Layer\")\n        # Long-term memory\n        self.LM = LongTermMemory(self.embedding_dim, activation=self.activation)\n\n        # Persistent memory vector (trainable, sequence-independent)\n        self.persistent_memory = self.add_weight(\n            shape=(1, 1, self.embedding_dim),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n            name=\"Persistent_Memory\"\n        )\n\n        # Normalization after concatenation\n        self.norm = LayerNormalization(epsilon=1e-6, name=\"Memory_Normalization_Layer\")\n\n    def call(self, inputs, mask=None):\n        q = self.LMWq(inputs)\n        ltm_out = self.LM(q)\n\n        batch_size = tf.shape(inputs)[0]\n        persistent = tf.tile(self.persistent_memory, [batch_size, self.sequence_length, 1])\n\n        concat = tf.concat([inputs, ltm_out, persistent], axis=-1)\n        norm = self.norm(concat)\n\n        return norm\n\n\n@keras.saving.register_keras_serializable()\nclass PositionwiseFeedforward(tf.keras.layers.Layer):\n    \"\"\"\n    Standard FFN (expansion + projection) used in Transformers.\n    \"\"\"\n    def __init__(self, embedding_dim, dff, **kwargs):\n        super(PositionwiseFeedforward, self).__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.dff = dff\n        self.name = \"Positionwise_Feedforward\"\n        self.dense1 = Dense(self.dff, activation='relu')\n        self.dense2 = Dense(self.embedding_dim)\n\n    def call(self, x):\n        return self.dense2(self.dense1(x))\n","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:15.374567Z","iopub.execute_input":"2025-08-29T14:24:15.374785Z","iopub.status.idle":"2025-08-29T14:24:15.396766Z","shell.execute_reply.started":"2025-08-29T14:24:15.374763Z","shell.execute_reply":"2025-08-29T14:24:15.396039Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"@keras.saving.register_keras_serializable()\nclass Titans(tf.keras.layers.Layer):\n    \"\"\"\n    Transformer-based memory-augmented architecture with masking support.\n    \"\"\"\n\n    def __init__(self, embedding_dim, sequence_length, num_heads, dff, vocab_size, \n                 rate=0.4, mask_zero=True, **kwargs):     \n        \"\"\"\n        Initializes the Titans layer.\n        Args:\n            embedding_dim (int): Dimensionality of the embedding space.\n            sequence_length (int): Length of the input sequences.\n            num_heads (int): Number of attention heads.\n            dff (int): Dimensionality of the feedforward network.\n            vocab_size (int): Total number of words in the vocabulary.\n            rate (float): Dropout rate.\n            mask_zero (bool): Whether to mask padding tokens.\n        Returns:\n            None\n        \"\"\"\n\n        super(Titans, self).__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.dff = dff\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.supports_masking = mask_zero\n        self.mask_zero = mask_zero\n        self.rate = rate\n        self.name = \"Titans\"\n\n    \n    def build(self, input_shape):\n        #Initializes layers (embedding, memory, attention, FFN, normalization, gating).\n        \n        # Embedding + positional encoding\n        self.embedding_layer = Embedding(\n            input_dim=self.vocab_size,   # should be vocab size, not seq length\n            output_dim=self.embedding_dim,\n            mask_zero=self.mask_zero\n        )\n\n        self.position_embedding = Embedding(\n            input_dim=self.sequence_length,\n            output_dim=self.embedding_dim\n        )\n\n        # Memory + Transformer components\n        self.memory = Memory(self.embedding_dim, self.sequence_length)\n        self.mha = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim)\n        self.ffn = PositionwiseFeedforward(self.embedding_dim * 3, self.dff)\n\n        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout = Dropout(self.rate)\n\n        # Gating\n        self.gate = Dense(units=self.embedding_dim * 3, activation='sigmoid')\n        self.modulation_layer = Dense(units=self.embedding_dim * 3)\n\n        # Final linear layer\n        self.final_layer = Dense(units=self.vocab_size)\n\n    def create_causal_mask(self,seq_len):\n        \"\"\"\n        Creates a causal mask for the given sequence length.\n        Args:\n            seq_len (int): Length of the sequence.\n        Returns:\n            tf.Tensor: Causal mask tensor.\n        \"\"\"\n        return tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n\n    def combine_masks(self,pad_mask, seq_len):\n        \"\"\"\n        Combines padding and causal masks.\n        Args:\n            pad_mask (tf.Tensor): Padding mask tensor.\n            seq_len (int): Length of the sequence.\n        Returns:\n            tf.Tensor: Combined mask tensor.\n        \"\"\"\n        causal_mask = self.create_causal_mask(seq_len)  # (seq_len, seq_len)\n        causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # (1,1,seq,seq)\n    \n        # pad mask → (batch, 1, 1, seq)\n        pad_mask = pad_mask[:, tf.newaxis, tf.newaxis, :]\n    \n        # Combine (broadcast AND)\n        return tf.cast(tf.logical_and(tf.cast(pad_mask, tf.bool),\n                                  tf.cast(causal_mask, tf.bool)), tf.float32)\n\n    def call(self, inputs, mask=None, training=False):\n\n        # Embedding\n        x = self.embedding_layer(inputs)\n    \n        # Padding mask\n        if mask is None:\n            mask = self.embedding_layer.compute_mask(inputs)   # (batch, seq_len)\n\n\n        # Attention mask\n        attn_mask = self.combine_masks(mask, self.sequence_length)  # (batch,1,seq,seq)\n        pad_mask = tf.cast(mask[:, :, tf.newaxis], x.dtype)  # (batch, seq_len, 1) for element-wise masking\n    \n        # Positional encoding\n        positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n        pos_emb = self.position_embedding(positions)\n        x = tf.add(x, pos_emb)\n\n        # Memory augmentation\n        memory_output = self.memory(x, mask=mask)\n        memory_output *= pad_mask   # ensure padding stays zero\n\n        # Multi-head attention\n        attn_output = self.mha(\n        memory_output,   # query\n        memory_output,   # value\n        memory_output,   # key\n        attention_mask=attn_mask,\n        training=training\n        )\n\n        attn_output *= pad_mask   # re-mask after MHA\n\n        # Feedforward\n        ffn_output = self.ffn(attn_output)\n        ffn_output = self.layernorm(ffn_output)\n        ffn_output = self.dropout(ffn_output, training=training)\n        ffn_output *= pad_mask   # re-mask after FFN + norm\n\n        # Skip connection\n        skip = tf.add(memory_output, ffn_output)\n        skip *= pad_mask   # ensure skip preserves masking\n\n        # Gating\n        linear_gating = self.gate(skip)\n        modulated_output = self.modulation_layer(linear_gating)\n        output = tf.multiply(linear_gating, modulated_output)\n        output *= pad_mask   # final mask application\n\n        # Final projection\n\n        return self.final_layer(output)   # logits over vocab\n    \n        \n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:15.397420Z","iopub.execute_input":"2025-08-29T14:24:15.397645Z","iopub.status.idle":"2025-08-29T14:24:15.418580Z","shell.execute_reply.started":"2025-08-29T14:24:15.397629Z","shell.execute_reply":"2025-08-29T14:24:15.417831Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Define a custom model using Titans Transformer-based architecture\n@keras.saving.register_keras_serializable()\nclass Model(tf.keras.Model):\n    def __init__(self, embedding_dim, sequence_length, num_heads, dff, vocab_size, **kwargs):\n        \"\"\"\n        Custom Transformer-based model using Titans library.\n        Args:\n            embedding_dim (int): Size of the word embedding.\n            sequence_length (int): Maximum length of input sequences.\n            num_heads (int): Number of attention heads.\n            dff (int): Dimension of the feed-forward network.\n            vocab_size (int): Vocabulary size.\n        \"\"\"\n        super(Model, self).__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.sequence_length = sequence_length\n        self.num_heads = num_heads\n        self.dff = dff\n        self.vocab_size = vocab_size\n        self.name = \"Model\"\n        # Builds the model layers, including the Titans module.\n        self.titans = Titans(embedding_dim=self.embedding_dim,\n                             sequence_length=self.sequence_length,\n                             num_heads=self.num_heads,\n                             dff=self.dff,\n                             vocab_size=self.vocab_size)\n\n\n    def call(self, inputs):\n        \"\"\"\n        Defines forward pass of the model.\n        \"\"\"\n        x = self.titans(inputs, mask=None)\n        return x\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embedding_dim\": self.embedding_dim,\n            \"sequence_length\": self.sequence_length,\n            \"num_heads\": self.num_heads,\n            \"dff\": self.dff,\n            \"vocab_size\": self.vocab_size\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:15.419353Z","iopub.execute_input":"2025-08-29T14:24:15.419567Z","iopub.status.idle":"2025-08-29T14:24:15.435930Z","shell.execute_reply.started":"2025-08-29T14:24:15.419545Z","shell.execute_reply":"2025-08-29T14:24:15.435160Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Reading the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(DATASET_PATH)","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:15.436683Z","iopub.execute_input":"2025-08-29T14:24:15.436912Z","iopub.status.idle":"2025-08-29T14:24:22.098909Z","shell.execute_reply.started":"2025-08-29T14:24:15.436891Z","shell.execute_reply":"2025-08-29T14:24:22.098324Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"sample_df = df.sample(50000)","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:22.101143Z","iopub.execute_input":"2025-08-29T14:24:22.101368Z","iopub.status.idle":"2025-08-29T14:24:22.236948Z","shell.execute_reply.started":"2025-08-29T14:24:22.101352Z","shell.execute_reply":"2025-08-29T14:24:22.236394Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Split into Train and Val\ntrain_df, val_df = train_test_split(sample_df, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:22.237765Z","iopub.execute_input":"2025-08-29T14:24:22.238049Z","iopub.status.idle":"2025-08-29T14:24:22.257701Z","shell.execute_reply.started":"2025-08-29T14:24:22.238024Z","shell.execute_reply":"2025-08-29T14:24:22.257074Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Preprocess Input Strings","metadata":{}},{"cell_type":"code","source":"def tokenize_selfies(smiles: str) -> list:\n    \"\"\"Tokenizes a SMILES string into SELFIES tokens.\n    Args:\n        smiles (str): A SMILES string.\n    Returns:\n        list: A list of SELFIES tokens.\n    \"\"\"\n\n    try:\n        selfies = sf.encoder(smiles)\n    except Exception:\n        selfies = \"\"   # skip invalid ones\n    return list(sf.split_selfies(selfies))\n\n\ndef build_vocab_from_dataframe(df: pd.DataFrame) -> list:\n    \"\"\"\n    Builds a vocabulary of SELFIES tokens directly from a DataFrame of SMILES.\n    Adds <start>, <end>, and <UNK> special tokens.\n    Args:\n        df (pd.DataFrame): A DataFrame containing SMILES strings.\n    Returns:\n        list: A list of unique SELFIES tokens.\n    \"\"\"\n    vocab = {\"<start>\", \"<end>\"}  # start with special tokens\n\n    for smiles in df['smiles']:\n        tokens = tokenize_selfies(smiles)\n        vocab.update(tokens)\n\n    return sorted(list(vocab))\n\n\ndef tokenizer_initialize_from_dataframe(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Initializes a Keras tokenizer using vocab built from DataFrame.\n    Args:\n        df (pd.DataFrame): A DataFrame containing SMILES strings.\n    Returns:\n        tuple: A tuple containing the tokenizer, vocabulary size, and all tokens.\n    \"\"\"\n    all_tokens = build_vocab_from_dataframe(df)\n\n    tokenizer = Tokenizer(oov_token=\"<UNK>\", filters='', lower=False)\n    tokenizer.fit_on_texts(all_tokens)\n\n    vocab_size = len(tokenizer.word_index) + 1 # +1 for padding (index 0)\n    return tokenizer, vocab_size, all_tokens\n\n\ndef sequence_generator(df: pd.DataFrame, tokenizer, max_seq_length: int, seq_padding: int = 1) -> tuple:\n    \"\"\"Generates sequences of token IDs from SELFIES with post-padding.\n    Args:\n        df (pd.DataFrame): A DataFrame containing SMILES strings.\n        tokenizer: A Keras tokenizer fitted on SELFIES tokens.\n        max_seq_length (int): The maximum sequence length.\n        seq_padding (int): The amount of padding to apply.\n    Yields:\n        tuple: A tuple containing the input and target sequences.\n    \"\"\"\n    for smiles in df['smiles']:\n        tokens = [\"<start>\"]\n        selfies = tokenize_selfies(smiles)\n        tokens.extend(selfies)\n        tokens.append(\"<end>\")\n\n        token_ids = tokenizer.texts_to_sequences([tokens])[0]\n\n        target_len = max_seq_length + seq_padding\n\n        # Post-pad sequences\n        if len(token_ids) < target_len:\n            token_ids = token_ids + [0] * (target_len - len(token_ids))\n        elif len(token_ids) > target_len:\n            token_ids = token_ids[-target_len:]  # truncate from the left if too long\n\n        x = token_ids[:-1]\n        y = token_ids[1:]\n        yield x, y\n\n\ndef create_selfies_dataset(sample_df: pd.DataFrame, max_seq_length: int, batch_size: int = 256, buffer_size: int = 10000, seq_padding: int = 1) -> tuple:\n    \"\"\"\n    Creates a TensorFlow dataset from SMILES strings using SELFIES encoding.\n    Args:\n        sample_df (pd.DataFrame): A DataFrame containing SMILES strings.\n        max_seq_length (int): The maximum sequence length.\n        batch_size (int): The batch size for the dataset.\n        buffer_size (int): The buffer size for shuffling.\n        seq_padding (int): The amount of padding to apply.\n    Returns:\n        tuple: A tuple containing the dataset, tokenizer, vocabulary size, maximum sequence length, and all tokens.\n    \"\"\"\n    tokenizer, vocab_size, all_tokens = tokenizer_initialize_from_dataframe(sample_df)\n\n    output_signature = (\n        tf.TensorSpec(shape=(max_seq_length,), dtype=tf.int32),  # Input sequence\n        tf.TensorSpec(shape=(max_seq_length,), dtype=tf.int32)   # Target sequence\n    )\n\n    dataset = tf.data.Dataset.from_generator(\n        lambda: sequence_generator(sample_df, tokenizer, max_seq_length),\n        output_signature=output_signature\n    )\n\n    dataset = dataset.shuffle(buffer_size).repeat().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    return dataset, tokenizer, vocab_size, max_seq_length, all_tokens\n\n\ndef dataset_gen(dataframe: pd.DataFrame, tokenizer, max_seq_length: int, vocab_size: int, batch_size: int = 128, buffer_size: int = 10000, seq_padding: int = 1) -> tf.data.Dataset:\n    \"\"\"Generates a TensorFlow dataset from a DataFrame using a given tokenizer.\n    Args:\n        dataframe (pd.DataFrame): A DataFrame containing SMILES strings.\n        tokenizer: A Keras tokenizer fitted on SELFIES tokens.\n        max_seq_length (int): The maximum sequence length.\n        vocab_size (int): The vocabulary size.\n        batch_size (int): The batch size for the dataset.\n        buffer_size (int): The buffer size for shuffling.\n        seq_padding (int): The amount of padding to apply.\n    Returns:\n        tf.data.Dataset: A TensorFlow dataset.\n    \"\"\"\n    output_signature = (\n        tf.TensorSpec(shape=(max_seq_length,), dtype=tf.int32),\n        tf.TensorSpec(shape=(max_seq_length,), dtype=tf.int32)\n    )\n\n    dataset = tf.data.Dataset.from_generator(\n        lambda: sequence_generator(dataframe, tokenizer, max_seq_length),\n        output_signature=output_signature\n    )\n\n    dataset = dataset.shuffle(buffer_size).repeat().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    return dataset\n\n\n# Create the training dataset\ndataset, tokenizer, vocab_size, max_seq_length, all_tokens = create_selfies_dataset(\n    train_df, max_seq_length=SEQ_LENGTH , batch_size=BATCH_SIZE\n)\n\n# Create the validation dataset\nval_dataset = dataset_gen(val_df, tokenizer, max_seq_length, vocab_size, batch_size=BATCH_SIZE)\n","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:22.258564Z","iopub.execute_input":"2025-08-29T14:24:22.258787Z","iopub.status.idle":"2025-08-29T14:24:38.355178Z","shell.execute_reply.started":"2025-08-29T14:24:22.258770Z","shell.execute_reply":"2025-08-29T14:24:38.354432Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Get token IDs for special tokens\nstart_token_id = tokenizer.word_index[\"<start>\"]\nend_token_id = tokenizer.word_index[\"<end>\"]\nprint(f\"<start>: {start_token_id}, <end>: {end_token_id}\")","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:38.356037Z","iopub.execute_input":"2025-08-29T14:24:38.356251Z","iopub.status.idle":"2025-08-29T14:24:38.360636Z","shell.execute_reply.started":"2025-08-29T14:24:38.356234Z","shell.execute_reply":"2025-08-29T14:24:38.360035Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<start>: 4, <end>: 3\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Initialize Model","metadata":{}},{"cell_type":"code","source":"model = Model(embedding_dim=EMBEDDING_DIM, sequence_length=SEQ_LENGTH, num_heads=NUM_HEADS, dff=DFF, vocab_size=vocab_size)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:38.361437Z","iopub.execute_input":"2025-08-29T14:24:38.361752Z","iopub.status.idle":"2025-08-29T14:24:38.378317Z","shell.execute_reply.started":"2025-08-29T14:24:38.361730Z","shell.execute_reply":"2025-08-29T14:24:38.377786Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Build by calling with input shape\ninputs = tf.keras.Input(shape=(SEQ_LENGTH,))\noutputs = model(inputs)\n\ndrug_discovery_model = tf.keras.Model(inputs, outputs, name=\"DrugDiscoveryModel\")\ndrug_discovery_model.compile(loss=masked_loss, optimizer=optimizer, metrics=metrics)\ndrug_discovery_model.summary()","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:38.379335Z","iopub.execute_input":"2025-08-29T14:24:38.379672Z","iopub.status.idle":"2025-08-29T14:24:39.792926Z","shell.execute_reply.started":"2025-08-29T14:24:38.379655Z","shell.execute_reply":"2025-08-29T14:24:39.792417Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"DrugDiscoveryModel\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"DrugDiscoveryModel\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Model (\u001b[38;5;33mModel\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m135\u001b[0m)        │     \u001b[38;5;34m9,602,951\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Model (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Model</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,602,951</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,602,951\u001b[0m (36.63 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,602,951</span> (36.63 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,602,951\u001b[0m (36.63 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,602,951</span> (36.63 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"\n\nSTEPS_PER_EPOCH = math.ceil(len(train_df) / BATCH_SIZE) # returns int \nVAL_STEPS = math.ceil(len(val_df) / BATCH_SIZE)\n\ncheckpoint_path = \"best_model.weights.h5\"\n\ncallbacks = [\n    ModelCheckpoint(\n        filepath=checkpoint_path,\n        monitor=\"sparse_categorical_accuracy\",\n        mode=\"max\",\n        save_best_only=True,\n        save_weights_only=True,\n        verbose=1\n    ),\n    EarlyStopping(\n        monitor=\"sparse_categorical_accuracy\",\n        mode=\"max\",\n        patience=10,\n        verbose=1,\n        restore_best_weights=True\n    ),\n    # Define LR scheduler\n    ReduceLROnPlateau(\n        monitor=\"val_loss\",     # quantity to be monitored\n        factor=0.5,             # new_lr = lr * factor\n        patience=3,             # wait for 3 epochs before reducing LR\n        min_lr=1e-6,            # lower bound on LR\n        verbose=1               # print LR updates\n    )\n\n]\n\nhistory = drug_discovery_model.fit(\n    dataset,\n    epochs=1000,\n    validation_data=val_dataset,\n    steps_per_epoch=int(STEPS_PER_EPOCH),\n    validation_steps=int(VAL_STEPS),\n    callbacks=callbacks\n)\n\ndrug_discovery_model.load_weights(checkpoint_path)\n\n# Save the model\ndrug_discovery_model.save(\"drug_discovery_model.keras\")","metadata":{"execution":{"iopub.status.busy":"2025-08-29T14:24:39.793623Z","iopub.execute_input":"2025-08-29T14:24:39.793887Z","iopub.status.idle":"2025-08-29T15:51:14.171742Z","shell.execute_reply.started":"2025-08-29T14:24:39.793864Z","shell.execute_reply":"2025-08-29T15:51:14.171109Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/1000\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1756477494.189630     113 service.cc:148] XLA service 0x7aa788006570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1756477494.192792     113 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nW0000 00:00:1756477495.052366     113 assert_op.cc:38] Ignoring Assert operator compile_loss/masked_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nI0000 00:00:1756477495.391004     113 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1756477511.681804     113 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765ms/step - loss: 3.0411 - sparse_categorical_accuracy: 0.2813","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1756477554.994068     112 assert_op.cc:38] Ignoring Assert operator compile_loss/masked_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: sparse_categorical_accuracy improved from -inf to 0.29760, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - loss: 3.0290 - sparse_categorical_accuracy: 0.2816 - val_loss: 2.0677 - val_sparse_categorical_accuracy: 0.3271 - learning_rate: 1.0000e-04\nEpoch 2/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772ms/step - loss: 2.0431 - sparse_categorical_accuracy: 0.3298\nEpoch 2: sparse_categorical_accuracy improved from 0.29760 to 0.33621, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - loss: 2.0421 - sparse_categorical_accuracy: 0.3300 - val_loss: 1.8721 - val_sparse_categorical_accuracy: 0.3549 - learning_rate: 1.0000e-04\nEpoch 3/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - loss: 1.8343 - sparse_categorical_accuracy: 0.3637\nEpoch 3: sparse_categorical_accuracy improved from 0.33621 to 0.37424, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - loss: 1.8332 - sparse_categorical_accuracy: 0.3640 - val_loss: 1.6633 - val_sparse_categorical_accuracy: 0.3952 - learning_rate: 1.0000e-04\nEpoch 4/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - loss: 1.6532 - sparse_categorical_accuracy: 0.3965\nEpoch 4: sparse_categorical_accuracy improved from 0.37424 to 0.40230, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - loss: 1.6526 - sparse_categorical_accuracy: 0.3966 - val_loss: 1.5437 - val_sparse_categorical_accuracy: 0.4189 - learning_rate: 1.0000e-04\nEpoch 5/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.5363 - sparse_categorical_accuracy: 0.4189\nEpoch 5: sparse_categorical_accuracy improved from 0.40230 to 0.42272, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.5359 - sparse_categorical_accuracy: 0.4190 - val_loss: 1.4867 - val_sparse_categorical_accuracy: 0.4245 - learning_rate: 1.0000e-04\nEpoch 6/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.4562 - sparse_categorical_accuracy: 0.4326\nEpoch 6: sparse_categorical_accuracy improved from 0.42272 to 0.43741, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 991ms/step - loss: 1.4558 - sparse_categorical_accuracy: 0.4327 - val_loss: 1.3670 - val_sparse_categorical_accuracy: 0.4507 - learning_rate: 1.0000e-04\nEpoch 7/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.3978 - sparse_categorical_accuracy: 0.4430\nEpoch 7: sparse_categorical_accuracy improved from 0.43741 to 0.44621, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 999ms/step - loss: 1.3975 - sparse_categorical_accuracy: 0.4431 - val_loss: 1.3164 - val_sparse_categorical_accuracy: 0.4595 - learning_rate: 1.0000e-04\nEpoch 8/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.3376 - sparse_categorical_accuracy: 0.4540\nEpoch 8: sparse_categorical_accuracy improved from 0.44621 to 0.45552, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 995ms/step - loss: 1.3375 - sparse_categorical_accuracy: 0.4541 - val_loss: 1.2943 - val_sparse_categorical_accuracy: 0.4619 - learning_rate: 1.0000e-04\nEpoch 9/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.3096 - sparse_categorical_accuracy: 0.4588\nEpoch 9: sparse_categorical_accuracy improved from 0.45552 to 0.46097, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.3095 - sparse_categorical_accuracy: 0.4589 - val_loss: 1.2652 - val_sparse_categorical_accuracy: 0.4687 - learning_rate: 1.0000e-04\nEpoch 10/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.2832 - sparse_categorical_accuracy: 0.4642\nEpoch 10: sparse_categorical_accuracy improved from 0.46097 to 0.46582, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 995ms/step - loss: 1.2831 - sparse_categorical_accuracy: 0.4643 - val_loss: 1.2368 - val_sparse_categorical_accuracy: 0.4736 - learning_rate: 1.0000e-04\nEpoch 11/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.2620 - sparse_categorical_accuracy: 0.4684\nEpoch 11: sparse_categorical_accuracy improved from 0.46582 to 0.46922, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 999ms/step - loss: 1.2619 - sparse_categorical_accuracy: 0.4684 - val_loss: 1.2178 - val_sparse_categorical_accuracy: 0.4780 - learning_rate: 1.0000e-04\nEpoch 12/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.2418 - sparse_categorical_accuracy: 0.4733\nEpoch 12: sparse_categorical_accuracy improved from 0.46922 to 0.47378, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.2418 - sparse_categorical_accuracy: 0.4733 - val_loss: 1.2135 - val_sparse_categorical_accuracy: 0.4784 - learning_rate: 1.0000e-04\nEpoch 13/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.2285 - sparse_categorical_accuracy: 0.4755\nEpoch 13: sparse_categorical_accuracy improved from 0.47378 to 0.47666, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 995ms/step - loss: 1.2284 - sparse_categorical_accuracy: 0.4755 - val_loss: 1.1917 - val_sparse_categorical_accuracy: 0.4843 - learning_rate: 1.0000e-04\nEpoch 14/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.2151 - sparse_categorical_accuracy: 0.4785\nEpoch 14: sparse_categorical_accuracy improved from 0.47666 to 0.47923, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 997ms/step - loss: 1.2151 - sparse_categorical_accuracy: 0.4785 - val_loss: 1.2167 - val_sparse_categorical_accuracy: 0.4766 - learning_rate: 1.0000e-04\nEpoch 15/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.2232 - sparse_categorical_accuracy: 0.4758\nEpoch 15: sparse_categorical_accuracy improved from 0.47923 to 0.47974, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.2229 - sparse_categorical_accuracy: 0.4759 - val_loss: 1.1713 - val_sparse_categorical_accuracy: 0.4885 - learning_rate: 1.0000e-04\nEpoch 16/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.1882 - sparse_categorical_accuracy: 0.4839\nEpoch 16: sparse_categorical_accuracy improved from 0.47974 to 0.48442, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.1882 - sparse_categorical_accuracy: 0.4839 - val_loss: 1.1634 - val_sparse_categorical_accuracy: 0.4898 - learning_rate: 1.0000e-04\nEpoch 17/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.1797 - sparse_categorical_accuracy: 0.4859\nEpoch 17: sparse_categorical_accuracy improved from 0.48442 to 0.48728, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.1796 - sparse_categorical_accuracy: 0.4859 - val_loss: 1.1553 - val_sparse_categorical_accuracy: 0.4916 - learning_rate: 1.0000e-04\nEpoch 18/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549ms/step - loss: 1.1723 - sparse_categorical_accuracy: 0.4874\nEpoch 18: sparse_categorical_accuracy did not improve from 0.48728\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.1724 - sparse_categorical_accuracy: 0.4873 - val_loss: 1.1481 - val_sparse_categorical_accuracy: 0.4930 - learning_rate: 1.0000e-04\nEpoch 19/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568ms/step - loss: 1.1596 - sparse_categorical_accuracy: 0.4889\nEpoch 19: sparse_categorical_accuracy improved from 0.48728 to 0.49042, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 1.1596 - sparse_categorical_accuracy: 0.4889 - val_loss: 1.1389 - val_sparse_categorical_accuracy: 0.4953 - learning_rate: 1.0000e-04\nEpoch 20/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.1550 - sparse_categorical_accuracy: 0.4914\nEpoch 20: sparse_categorical_accuracy improved from 0.49042 to 0.49145, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.1550 - sparse_categorical_accuracy: 0.4914 - val_loss: 1.1351 - val_sparse_categorical_accuracy: 0.4969 - learning_rate: 1.0000e-04\nEpoch 21/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.1484 - sparse_categorical_accuracy: 0.4921\nEpoch 21: sparse_categorical_accuracy improved from 0.49145 to 0.49358, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.1483 - sparse_categorical_accuracy: 0.4921 - val_loss: 1.1271 - val_sparse_categorical_accuracy: 0.4974 - learning_rate: 1.0000e-04\nEpoch 22/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.1634 - sparse_categorical_accuracy: 0.4893\nEpoch 22: sparse_categorical_accuracy did not improve from 0.49358\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 993ms/step - loss: 1.1636 - sparse_categorical_accuracy: 0.4893 - val_loss: 1.1296 - val_sparse_categorical_accuracy: 0.4979 - learning_rate: 1.0000e-04\nEpoch 23/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.1359 - sparse_categorical_accuracy: 0.4955\nEpoch 23: sparse_categorical_accuracy improved from 0.49358 to 0.49633, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.1359 - sparse_categorical_accuracy: 0.4955 - val_loss: 1.1207 - val_sparse_categorical_accuracy: 0.4997 - learning_rate: 1.0000e-04\nEpoch 24/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.1259 - sparse_categorical_accuracy: 0.4971\nEpoch 24: sparse_categorical_accuracy improved from 0.49633 to 0.49811, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.1259 - sparse_categorical_accuracy: 0.4971 - val_loss: 1.1149 - val_sparse_categorical_accuracy: 0.5007 - learning_rate: 1.0000e-04\nEpoch 25/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.1209 - sparse_categorical_accuracy: 0.4988\nEpoch 25: sparse_categorical_accuracy improved from 0.49811 to 0.49906, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.1209 - sparse_categorical_accuracy: 0.4988 - val_loss: 1.1099 - val_sparse_categorical_accuracy: 0.5015 - learning_rate: 1.0000e-04\nEpoch 26/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.1170 - sparse_categorical_accuracy: 0.4994\nEpoch 26: sparse_categorical_accuracy improved from 0.49906 to 0.50025, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.1170 - sparse_categorical_accuracy: 0.4995 - val_loss: 1.1005 - val_sparse_categorical_accuracy: 0.5043 - learning_rate: 1.0000e-04\nEpoch 27/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.1091 - sparse_categorical_accuracy: 0.5006\nEpoch 27: sparse_categorical_accuracy improved from 0.50025 to 0.50136, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.1091 - sparse_categorical_accuracy: 0.5006 - val_loss: 1.1134 - val_sparse_categorical_accuracy: 0.4995 - learning_rate: 1.0000e-04\nEpoch 28/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.1105 - sparse_categorical_accuracy: 0.5000\nEpoch 28: sparse_categorical_accuracy did not improve from 0.50136\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 993ms/step - loss: 1.1104 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0960 - val_sparse_categorical_accuracy: 0.5047 - learning_rate: 1.0000e-04\nEpoch 29/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0989 - sparse_categorical_accuracy: 0.5028\nEpoch 29: sparse_categorical_accuracy improved from 0.50136 to 0.50337, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0989 - sparse_categorical_accuracy: 0.5028 - val_loss: 1.0913 - val_sparse_categorical_accuracy: 0.5054 - learning_rate: 1.0000e-04\nEpoch 30/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0960 - sparse_categorical_accuracy: 0.5039\nEpoch 30: sparse_categorical_accuracy improved from 0.50337 to 0.50407, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0961 - sparse_categorical_accuracy: 0.5039 - val_loss: 1.0870 - val_sparse_categorical_accuracy: 0.5075 - learning_rate: 1.0000e-04\nEpoch 31/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0909 - sparse_categorical_accuracy: 0.5039\nEpoch 31: sparse_categorical_accuracy improved from 0.50407 to 0.50514, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0909 - sparse_categorical_accuracy: 0.5040 - val_loss: 1.0866 - val_sparse_categorical_accuracy: 0.5076 - learning_rate: 1.0000e-04\nEpoch 32/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0888 - sparse_categorical_accuracy: 0.5049\nEpoch 32: sparse_categorical_accuracy improved from 0.50514 to 0.50600, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0888 - sparse_categorical_accuracy: 0.5049 - val_loss: 1.0821 - val_sparse_categorical_accuracy: 0.5071 - learning_rate: 1.0000e-04\nEpoch 33/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0830 - sparse_categorical_accuracy: 0.5063\nEpoch 33: sparse_categorical_accuracy improved from 0.50600 to 0.50741, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0830 - sparse_categorical_accuracy: 0.5063 - val_loss: 1.0753 - val_sparse_categorical_accuracy: 0.5100 - learning_rate: 1.0000e-04\nEpoch 34/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0744 - sparse_categorical_accuracy: 0.5087\nEpoch 34: sparse_categorical_accuracy improved from 0.50741 to 0.50860, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0745 - sparse_categorical_accuracy: 0.5086 - val_loss: 1.0743 - val_sparse_categorical_accuracy: 0.5093 - learning_rate: 1.0000e-04\nEpoch 35/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0785 - sparse_categorical_accuracy: 0.5077\nEpoch 35: sparse_categorical_accuracy did not improve from 0.50860\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 992ms/step - loss: 1.0785 - sparse_categorical_accuracy: 0.5077 - val_loss: 1.0747 - val_sparse_categorical_accuracy: 0.5097 - learning_rate: 1.0000e-04\nEpoch 36/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0725 - sparse_categorical_accuracy: 0.5082\nEpoch 36: sparse_categorical_accuracy improved from 0.50860 to 0.50905, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0725 - sparse_categorical_accuracy: 0.5082 - val_loss: 1.0685 - val_sparse_categorical_accuracy: 0.5109 - learning_rate: 1.0000e-04\nEpoch 37/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0663 - sparse_categorical_accuracy: 0.5101\nEpoch 37: sparse_categorical_accuracy improved from 0.50905 to 0.51028, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 998ms/step - loss: 1.0663 - sparse_categorical_accuracy: 0.5101 - val_loss: 1.0769 - val_sparse_categorical_accuracy: 0.5088 - learning_rate: 1.0000e-04\nEpoch 38/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0639 - sparse_categorical_accuracy: 0.5100\nEpoch 38: sparse_categorical_accuracy improved from 0.51028 to 0.51039, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0639 - sparse_categorical_accuracy: 0.5100 - val_loss: 1.0672 - val_sparse_categorical_accuracy: 0.5109 - learning_rate: 1.0000e-04\nEpoch 39/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0621 - sparse_categorical_accuracy: 0.5106\nEpoch 39: sparse_categorical_accuracy improved from 0.51039 to 0.51113, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0621 - sparse_categorical_accuracy: 0.5106 - val_loss: 1.0664 - val_sparse_categorical_accuracy: 0.5118 - learning_rate: 1.0000e-04\nEpoch 40/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0558 - sparse_categorical_accuracy: 0.5119\nEpoch 40: sparse_categorical_accuracy improved from 0.51113 to 0.51241, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0558 - sparse_categorical_accuracy: 0.5119 - val_loss: 1.0596 - val_sparse_categorical_accuracy: 0.5122 - learning_rate: 1.0000e-04\nEpoch 41/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0522 - sparse_categorical_accuracy: 0.5124\nEpoch 41: sparse_categorical_accuracy improved from 0.51241 to 0.51314, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0523 - sparse_categorical_accuracy: 0.5124 - val_loss: 1.0663 - val_sparse_categorical_accuracy: 0.5110 - learning_rate: 1.0000e-04\nEpoch 42/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - loss: 1.0520 - sparse_categorical_accuracy: 0.5128\nEpoch 42: sparse_categorical_accuracy improved from 0.51314 to 0.51343, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0520 - sparse_categorical_accuracy: 0.5128 - val_loss: 1.0558 - val_sparse_categorical_accuracy: 0.5138 - learning_rate: 1.0000e-04\nEpoch 43/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0447 - sparse_categorical_accuracy: 0.5141\nEpoch 43: sparse_categorical_accuracy improved from 0.51343 to 0.51466, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0448 - sparse_categorical_accuracy: 0.5141 - val_loss: 1.0568 - val_sparse_categorical_accuracy: 0.5132 - learning_rate: 1.0000e-04\nEpoch 44/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543ms/step - loss: 1.0452 - sparse_categorical_accuracy: 0.5148\nEpoch 44: sparse_categorical_accuracy improved from 0.51466 to 0.51517, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0452 - sparse_categorical_accuracy: 0.5148 - val_loss: 1.0516 - val_sparse_categorical_accuracy: 0.5152 - learning_rate: 1.0000e-04\nEpoch 45/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0430 - sparse_categorical_accuracy: 0.5154\nEpoch 45: sparse_categorical_accuracy improved from 0.51517 to 0.51561, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0431 - sparse_categorical_accuracy: 0.5154 - val_loss: 1.0528 - val_sparse_categorical_accuracy: 0.5126 - learning_rate: 1.0000e-04\nEpoch 46/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0374 - sparse_categorical_accuracy: 0.5155\nEpoch 46: sparse_categorical_accuracy improved from 0.51561 to 0.51621, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0374 - sparse_categorical_accuracy: 0.5155 - val_loss: 1.0531 - val_sparse_categorical_accuracy: 0.5146 - learning_rate: 1.0000e-04\nEpoch 47/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0366 - sparse_categorical_accuracy: 0.5160\nEpoch 47: sparse_categorical_accuracy improved from 0.51621 to 0.51647, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0367 - sparse_categorical_accuracy: 0.5160 - val_loss: 1.0494 - val_sparse_categorical_accuracy: 0.5144 - learning_rate: 1.0000e-04\nEpoch 48/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0372 - sparse_categorical_accuracy: 0.5165\nEpoch 48: sparse_categorical_accuracy improved from 0.51647 to 0.51673, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0372 - sparse_categorical_accuracy: 0.5165 - val_loss: 1.0458 - val_sparse_categorical_accuracy: 0.5152 - learning_rate: 1.0000e-04\nEpoch 49/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - loss: 1.0311 - sparse_categorical_accuracy: 0.5171\nEpoch 49: sparse_categorical_accuracy improved from 0.51673 to 0.51797, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 1.0311 - sparse_categorical_accuracy: 0.5172 - val_loss: 1.0430 - val_sparse_categorical_accuracy: 0.5172 - learning_rate: 1.0000e-04\nEpoch 50/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0259 - sparse_categorical_accuracy: 0.5191\nEpoch 50: sparse_categorical_accuracy improved from 0.51797 to 0.51883, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 1.0259 - sparse_categorical_accuracy: 0.5190 - val_loss: 1.0422 - val_sparse_categorical_accuracy: 0.5164 - learning_rate: 1.0000e-04\nEpoch 51/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0267 - sparse_categorical_accuracy: 0.5181\nEpoch 51: sparse_categorical_accuracy did not improve from 0.51883\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0267 - sparse_categorical_accuracy: 0.5181 - val_loss: 1.0459 - val_sparse_categorical_accuracy: 0.5156 - learning_rate: 1.0000e-04\nEpoch 52/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - loss: 1.0237 - sparse_categorical_accuracy: 0.5192\nEpoch 52: sparse_categorical_accuracy improved from 0.51883 to 0.51963, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 1.0237 - sparse_categorical_accuracy: 0.5192 - val_loss: 1.0410 - val_sparse_categorical_accuracy: 0.5171 - learning_rate: 1.0000e-04\nEpoch 53/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0222 - sparse_categorical_accuracy: 0.5205\nEpoch 53: sparse_categorical_accuracy improved from 0.51963 to 0.52049, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 1.0222 - sparse_categorical_accuracy: 0.5205 - val_loss: 1.0456 - val_sparse_categorical_accuracy: 0.5153 - learning_rate: 1.0000e-04\nEpoch 54/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0288 - sparse_categorical_accuracy: 0.5174\nEpoch 54: sparse_categorical_accuracy did not improve from 0.52049\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0288 - sparse_categorical_accuracy: 0.5174 - val_loss: 1.0365 - val_sparse_categorical_accuracy: 0.5182 - learning_rate: 1.0000e-04\nEpoch 55/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0145 - sparse_categorical_accuracy: 0.5219\nEpoch 55: sparse_categorical_accuracy improved from 0.52049 to 0.52186, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0145 - sparse_categorical_accuracy: 0.5219 - val_loss: 1.0349 - val_sparse_categorical_accuracy: 0.5181 - learning_rate: 1.0000e-04\nEpoch 56/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0104 - sparse_categorical_accuracy: 0.5226\nEpoch 56: sparse_categorical_accuracy improved from 0.52186 to 0.52234, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 1.0105 - sparse_categorical_accuracy: 0.5226 - val_loss: 1.0340 - val_sparse_categorical_accuracy: 0.5180 - learning_rate: 1.0000e-04\nEpoch 57/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - loss: 1.0111 - sparse_categorical_accuracy: 0.5216\nEpoch 57: sparse_categorical_accuracy improved from 0.52234 to 0.52249, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - loss: 1.0111 - sparse_categorical_accuracy: 0.5217 - val_loss: 1.0335 - val_sparse_categorical_accuracy: 0.5192 - learning_rate: 1.0000e-04\nEpoch 58/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546ms/step - loss: 1.0076 - sparse_categorical_accuracy: 0.5228\nEpoch 58: sparse_categorical_accuracy improved from 0.52249 to 0.52321, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 1.0076 - sparse_categorical_accuracy: 0.5228 - val_loss: 1.0342 - val_sparse_categorical_accuracy: 0.5193 - learning_rate: 1.0000e-04\nEpoch 59/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0087 - sparse_categorical_accuracy: 0.5227\nEpoch 59: sparse_categorical_accuracy did not improve from 0.52321\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0087 - sparse_categorical_accuracy: 0.5227 - val_loss: 1.0325 - val_sparse_categorical_accuracy: 0.5195 - learning_rate: 1.0000e-04\nEpoch 60/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0028 - sparse_categorical_accuracy: 0.5239\nEpoch 60: sparse_categorical_accuracy improved from 0.52321 to 0.52414, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0029 - sparse_categorical_accuracy: 0.5239 - val_loss: 1.0316 - val_sparse_categorical_accuracy: 0.5192 - learning_rate: 1.0000e-04\nEpoch 61/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0022 - sparse_categorical_accuracy: 0.5239\nEpoch 61: sparse_categorical_accuracy improved from 0.52414 to 0.52432, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0023 - sparse_categorical_accuracy: 0.5239 - val_loss: 1.0319 - val_sparse_categorical_accuracy: 0.5189 - learning_rate: 1.0000e-04\nEpoch 62/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.0039 - sparse_categorical_accuracy: 0.5238\nEpoch 62: sparse_categorical_accuracy did not improve from 0.52432\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 1.0039 - sparse_categorical_accuracy: 0.5238 - val_loss: 1.0350 - val_sparse_categorical_accuracy: 0.5192 - learning_rate: 1.0000e-04\nEpoch 63/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 1.0008 - sparse_categorical_accuracy: 0.5241\nEpoch 63: sparse_categorical_accuracy improved from 0.52432 to 0.52509, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 1.0008 - sparse_categorical_accuracy: 0.5241 - val_loss: 1.0274 - val_sparse_categorical_accuracy: 0.5210 - learning_rate: 1.0000e-04\nEpoch 64/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9945 - sparse_categorical_accuracy: 0.5247\nEpoch 64: sparse_categorical_accuracy improved from 0.52509 to 0.52572, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9945 - sparse_categorical_accuracy: 0.5247 - val_loss: 1.0265 - val_sparse_categorical_accuracy: 0.5210 - learning_rate: 1.0000e-04\nEpoch 65/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9923 - sparse_categorical_accuracy: 0.5259\nEpoch 65: sparse_categorical_accuracy improved from 0.52572 to 0.52594, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9924 - sparse_categorical_accuracy: 0.5259 - val_loss: 1.0260 - val_sparse_categorical_accuracy: 0.5198 - learning_rate: 1.0000e-04\nEpoch 66/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9905 - sparse_categorical_accuracy: 0.5267\nEpoch 66: sparse_categorical_accuracy improved from 0.52594 to 0.52685, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9906 - sparse_categorical_accuracy: 0.5267 - val_loss: 1.0267 - val_sparse_categorical_accuracy: 0.5203 - learning_rate: 1.0000e-04\nEpoch 67/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9919 - sparse_categorical_accuracy: 0.5257\nEpoch 67: sparse_categorical_accuracy did not improve from 0.52685\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9920 - sparse_categorical_accuracy: 0.5258 - val_loss: 1.0239 - val_sparse_categorical_accuracy: 0.5215 - learning_rate: 1.0000e-04\nEpoch 68/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9890 - sparse_categorical_accuracy: 0.5269\nEpoch 68: sparse_categorical_accuracy improved from 0.52685 to 0.52783, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9889 - sparse_categorical_accuracy: 0.5269 - val_loss: 1.0252 - val_sparse_categorical_accuracy: 0.5210 - learning_rate: 1.0000e-04\nEpoch 69/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9878 - sparse_categorical_accuracy: 0.5274\nEpoch 69: sparse_categorical_accuracy improved from 0.52783 to 0.52791, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9877 - sparse_categorical_accuracy: 0.5274 - val_loss: 1.0234 - val_sparse_categorical_accuracy: 0.5206 - learning_rate: 1.0000e-04\nEpoch 70/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9825 - sparse_categorical_accuracy: 0.5288\nEpoch 70: sparse_categorical_accuracy improved from 0.52791 to 0.52867, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9826 - sparse_categorical_accuracy: 0.5288 - val_loss: 1.0221 - val_sparse_categorical_accuracy: 0.5212 - learning_rate: 1.0000e-04\nEpoch 71/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9801 - sparse_categorical_accuracy: 0.5289\nEpoch 71: sparse_categorical_accuracy improved from 0.52867 to 0.52897, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9801 - sparse_categorical_accuracy: 0.5289 - val_loss: 1.0271 - val_sparse_categorical_accuracy: 0.5195 - learning_rate: 1.0000e-04\nEpoch 72/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9816 - sparse_categorical_accuracy: 0.5284\nEpoch 72: sparse_categorical_accuracy improved from 0.52897 to 0.52915, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9816 - sparse_categorical_accuracy: 0.5284 - val_loss: 1.0212 - val_sparse_categorical_accuracy: 0.5220 - learning_rate: 1.0000e-04\nEpoch 73/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9755 - sparse_categorical_accuracy: 0.5293\nEpoch 73: sparse_categorical_accuracy improved from 0.52915 to 0.53002, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9755 - sparse_categorical_accuracy: 0.5293 - val_loss: 1.0199 - val_sparse_categorical_accuracy: 0.5219 - learning_rate: 1.0000e-04\nEpoch 74/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9732 - sparse_categorical_accuracy: 0.5302\nEpoch 74: sparse_categorical_accuracy improved from 0.53002 to 0.53038, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9733 - sparse_categorical_accuracy: 0.5302 - val_loss: 1.0168 - val_sparse_categorical_accuracy: 0.5223 - learning_rate: 1.0000e-04\nEpoch 75/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9728 - sparse_categorical_accuracy: 0.5307\nEpoch 75: sparse_categorical_accuracy improved from 0.53038 to 0.53061, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9729 - sparse_categorical_accuracy: 0.5307 - val_loss: 1.0207 - val_sparse_categorical_accuracy: 0.5232 - learning_rate: 1.0000e-04\nEpoch 76/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9734 - sparse_categorical_accuracy: 0.5305\nEpoch 76: sparse_categorical_accuracy did not improve from 0.53061\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 995ms/step - loss: 0.9734 - sparse_categorical_accuracy: 0.5305 - val_loss: 1.0203 - val_sparse_categorical_accuracy: 0.5209 - learning_rate: 1.0000e-04\nEpoch 77/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9707 - sparse_categorical_accuracy: 0.5306\nEpoch 77: sparse_categorical_accuracy improved from 0.53061 to 0.53106, saving model to best_model.weights.h5\n\nEpoch 77: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9708 - sparse_categorical_accuracy: 0.5306 - val_loss: 1.0208 - val_sparse_categorical_accuracy: 0.5221 - learning_rate: 1.0000e-04\nEpoch 78/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9608 - sparse_categorical_accuracy: 0.5335\nEpoch 78: sparse_categorical_accuracy improved from 0.53106 to 0.53431, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9607 - sparse_categorical_accuracy: 0.5335 - val_loss: 1.0115 - val_sparse_categorical_accuracy: 0.5247 - learning_rate: 5.0000e-05\nEpoch 79/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9531 - sparse_categorical_accuracy: 0.5358\nEpoch 79: sparse_categorical_accuracy improved from 0.53431 to 0.53563, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9531 - sparse_categorical_accuracy: 0.5358 - val_loss: 1.0107 - val_sparse_categorical_accuracy: 0.5242 - learning_rate: 5.0000e-05\nEpoch 80/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9530 - sparse_categorical_accuracy: 0.5350\nEpoch 80: sparse_categorical_accuracy did not improve from 0.53563\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9530 - sparse_categorical_accuracy: 0.5350 - val_loss: 1.0099 - val_sparse_categorical_accuracy: 0.5253 - learning_rate: 5.0000e-05\nEpoch 81/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9516 - sparse_categorical_accuracy: 0.5356\nEpoch 81: sparse_categorical_accuracy improved from 0.53563 to 0.53572, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9516 - sparse_categorical_accuracy: 0.5356 - val_loss: 1.0089 - val_sparse_categorical_accuracy: 0.5255 - learning_rate: 5.0000e-05\nEpoch 82/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9506 - sparse_categorical_accuracy: 0.5358\nEpoch 82: sparse_categorical_accuracy improved from 0.53572 to 0.53658, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9506 - sparse_categorical_accuracy: 0.5358 - val_loss: 1.0121 - val_sparse_categorical_accuracy: 0.5240 - learning_rate: 5.0000e-05\nEpoch 83/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9503 - sparse_categorical_accuracy: 0.5354\nEpoch 83: sparse_categorical_accuracy did not improve from 0.53658\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9503 - sparse_categorical_accuracy: 0.5354 - val_loss: 1.0113 - val_sparse_categorical_accuracy: 0.5252 - learning_rate: 5.0000e-05\nEpoch 84/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9477 - sparse_categorical_accuracy: 0.5366\nEpoch 84: sparse_categorical_accuracy improved from 0.53658 to 0.53678, saving model to best_model.weights.h5\n\nEpoch 84: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9478 - sparse_categorical_accuracy: 0.5366 - val_loss: 1.0096 - val_sparse_categorical_accuracy: 0.5255 - learning_rate: 5.0000e-05\nEpoch 85/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9431 - sparse_categorical_accuracy: 0.5375\nEpoch 85: sparse_categorical_accuracy improved from 0.53678 to 0.53780, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9431 - sparse_categorical_accuracy: 0.5375 - val_loss: 1.0055 - val_sparse_categorical_accuracy: 0.5263 - learning_rate: 2.5000e-05\nEpoch 86/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9400 - sparse_categorical_accuracy: 0.5389\nEpoch 86: sparse_categorical_accuracy improved from 0.53780 to 0.53949, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - loss: 0.9400 - sparse_categorical_accuracy: 0.5389 - val_loss: 1.0055 - val_sparse_categorical_accuracy: 0.5261 - learning_rate: 2.5000e-05\nEpoch 87/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9385 - sparse_categorical_accuracy: 0.5377\nEpoch 87: sparse_categorical_accuracy did not improve from 0.53949\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9385 - sparse_categorical_accuracy: 0.5378 - val_loss: 1.0065 - val_sparse_categorical_accuracy: 0.5267 - learning_rate: 2.5000e-05\nEpoch 88/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9363 - sparse_categorical_accuracy: 0.5400\nEpoch 88: sparse_categorical_accuracy did not improve from 0.53949\n\nEpoch 88: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9364 - sparse_categorical_accuracy: 0.5400 - val_loss: 1.0071 - val_sparse_categorical_accuracy: 0.5262 - learning_rate: 2.5000e-05\nEpoch 89/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9352 - sparse_categorical_accuracy: 0.5397\nEpoch 89: sparse_categorical_accuracy improved from 0.53949 to 0.54027, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9352 - sparse_categorical_accuracy: 0.5397 - val_loss: 1.0051 - val_sparse_categorical_accuracy: 0.5265 - learning_rate: 1.2500e-05\nEpoch 90/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9353 - sparse_categorical_accuracy: 0.5399\nEpoch 90: sparse_categorical_accuracy improved from 0.54027 to 0.54045, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9353 - sparse_categorical_accuracy: 0.5399 - val_loss: 1.0042 - val_sparse_categorical_accuracy: 0.5269 - learning_rate: 1.2500e-05\nEpoch 91/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9321 - sparse_categorical_accuracy: 0.5393\nEpoch 91: sparse_categorical_accuracy did not improve from 0.54045\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9322 - sparse_categorical_accuracy: 0.5393 - val_loss: 1.0053 - val_sparse_categorical_accuracy: 0.5260 - learning_rate: 1.2500e-05\nEpoch 92/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9314 - sparse_categorical_accuracy: 0.5405\nEpoch 92: sparse_categorical_accuracy did not improve from 0.54045\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9315 - sparse_categorical_accuracy: 0.5405 - val_loss: 1.0031 - val_sparse_categorical_accuracy: 0.5274 - learning_rate: 1.2500e-05\nEpoch 93/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9314 - sparse_categorical_accuracy: 0.5404\nEpoch 93: sparse_categorical_accuracy improved from 0.54045 to 0.54070, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9314 - sparse_categorical_accuracy: 0.5405 - val_loss: 1.0054 - val_sparse_categorical_accuracy: 0.5266 - learning_rate: 1.2500e-05\nEpoch 94/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9324 - sparse_categorical_accuracy: 0.5401\nEpoch 94: sparse_categorical_accuracy did not improve from 0.54070\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9324 - sparse_categorical_accuracy: 0.5401 - val_loss: 1.0046 - val_sparse_categorical_accuracy: 0.5270 - learning_rate: 1.2500e-05\nEpoch 95/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - loss: 0.9320 - sparse_categorical_accuracy: 0.5398\nEpoch 95: sparse_categorical_accuracy did not improve from 0.54070\n\nEpoch 95: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9320 - sparse_categorical_accuracy: 0.5398 - val_loss: 1.0060 - val_sparse_categorical_accuracy: 0.5267 - learning_rate: 1.2500e-05\nEpoch 96/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9309 - sparse_categorical_accuracy: 0.5413\nEpoch 96: sparse_categorical_accuracy improved from 0.54070 to 0.54104, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9309 - sparse_categorical_accuracy: 0.5413 - val_loss: 1.0049 - val_sparse_categorical_accuracy: 0.5269 - learning_rate: 6.2500e-06\nEpoch 97/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9298 - sparse_categorical_accuracy: 0.5418\nEpoch 97: sparse_categorical_accuracy improved from 0.54104 to 0.54147, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9298 - sparse_categorical_accuracy: 0.5418 - val_loss: 1.0043 - val_sparse_categorical_accuracy: 0.5273 - learning_rate: 6.2500e-06\nEpoch 98/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9291 - sparse_categorical_accuracy: 0.5417\nEpoch 98: sparse_categorical_accuracy did not improve from 0.54147\n\nEpoch 98: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 998ms/step - loss: 0.9292 - sparse_categorical_accuracy: 0.5417 - val_loss: 1.0044 - val_sparse_categorical_accuracy: 0.5266 - learning_rate: 6.2500e-06\nEpoch 99/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9285 - sparse_categorical_accuracy: 0.5416\nEpoch 99: sparse_categorical_accuracy improved from 0.54147 to 0.54153, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9286 - sparse_categorical_accuracy: 0.5416 - val_loss: 1.0048 - val_sparse_categorical_accuracy: 0.5267 - learning_rate: 3.1250e-06\nEpoch 100/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543ms/step - loss: 0.9272 - sparse_categorical_accuracy: 0.5418\nEpoch 100: sparse_categorical_accuracy improved from 0.54153 to 0.54197, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9272 - sparse_categorical_accuracy: 0.5418 - val_loss: 1.0032 - val_sparse_categorical_accuracy: 0.5271 - learning_rate: 3.1250e-06\nEpoch 101/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9285 - sparse_categorical_accuracy: 0.5419\nEpoch 101: sparse_categorical_accuracy did not improve from 0.54197\n\nEpoch 101: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 992ms/step - loss: 0.9285 - sparse_categorical_accuracy: 0.5419 - val_loss: 1.0052 - val_sparse_categorical_accuracy: 0.5264 - learning_rate: 3.1250e-06\nEpoch 102/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9279 - sparse_categorical_accuracy: 0.5420\nEpoch 102: sparse_categorical_accuracy did not improve from 0.54197\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9279 - sparse_categorical_accuracy: 0.5420 - val_loss: 1.0029 - val_sparse_categorical_accuracy: 0.5273 - learning_rate: 1.5625e-06\nEpoch 103/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9272 - sparse_categorical_accuracy: 0.5418\nEpoch 103: sparse_categorical_accuracy did not improve from 0.54197\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 998ms/step - loss: 0.9272 - sparse_categorical_accuracy: 0.5418 - val_loss: 1.0033 - val_sparse_categorical_accuracy: 0.5271 - learning_rate: 1.5625e-06\nEpoch 104/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9283 - sparse_categorical_accuracy: 0.5421\nEpoch 104: sparse_categorical_accuracy improved from 0.54197 to 0.54200, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9284 - sparse_categorical_accuracy: 0.5421 - val_loss: 1.0052 - val_sparse_categorical_accuracy: 0.5267 - learning_rate: 1.5625e-06\nEpoch 105/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9267 - sparse_categorical_accuracy: 0.5429\nEpoch 105: sparse_categorical_accuracy did not improve from 0.54200\n\nEpoch 105: ReduceLROnPlateau reducing learning rate to 1e-06.\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 996ms/step - loss: 0.9267 - sparse_categorical_accuracy: 0.5429 - val_loss: 1.0039 - val_sparse_categorical_accuracy: 0.5270 - learning_rate: 1.5625e-06\nEpoch 106/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9264 - sparse_categorical_accuracy: 0.5433\nEpoch 106: sparse_categorical_accuracy improved from 0.54200 to 0.54263, saving model to best_model.weights.h5\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - loss: 0.9264 - sparse_categorical_accuracy: 0.5433 - val_loss: 1.0032 - val_sparse_categorical_accuracy: 0.5269 - learning_rate: 1.0000e-06\nEpoch 107/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 0.9266 - sparse_categorical_accuracy: 0.5417\nEpoch 107: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 989ms/step - loss: 0.9267 - sparse_categorical_accuracy: 0.5417 - val_loss: 1.0039 - val_sparse_categorical_accuracy: 0.5273 - learning_rate: 1.0000e-06\nEpoch 108/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9247 - sparse_categorical_accuracy: 0.5425\nEpoch 108: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 998ms/step - loss: 0.9248 - sparse_categorical_accuracy: 0.5425 - val_loss: 1.0031 - val_sparse_categorical_accuracy: 0.5271 - learning_rate: 1.0000e-06\nEpoch 109/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9282 - sparse_categorical_accuracy: 0.5425\nEpoch 109: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9282 - sparse_categorical_accuracy: 0.5425 - val_loss: 1.0034 - val_sparse_categorical_accuracy: 0.5269 - learning_rate: 1.0000e-06\nEpoch 110/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9273 - sparse_categorical_accuracy: 0.5419\nEpoch 110: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 996ms/step - loss: 0.9273 - sparse_categorical_accuracy: 0.5419 - val_loss: 1.0049 - val_sparse_categorical_accuracy: 0.5268 - learning_rate: 1.0000e-06\nEpoch 111/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9272 - sparse_categorical_accuracy: 0.5422\nEpoch 111: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 996ms/step - loss: 0.9272 - sparse_categorical_accuracy: 0.5422 - val_loss: 1.0041 - val_sparse_categorical_accuracy: 0.5267 - learning_rate: 1.0000e-06\nEpoch 112/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9275 - sparse_categorical_accuracy: 0.5413\nEpoch 112: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9275 - sparse_categorical_accuracy: 0.5413 - val_loss: 1.0033 - val_sparse_categorical_accuracy: 0.5271 - learning_rate: 1.0000e-06\nEpoch 113/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9252 - sparse_categorical_accuracy: 0.5426\nEpoch 113: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 994ms/step - loss: 0.9253 - sparse_categorical_accuracy: 0.5426 - val_loss: 1.0028 - val_sparse_categorical_accuracy: 0.5275 - learning_rate: 1.0000e-06\nEpoch 114/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546ms/step - loss: 0.9266 - sparse_categorical_accuracy: 0.5417\nEpoch 114: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - loss: 0.9266 - sparse_categorical_accuracy: 0.5417 - val_loss: 1.0030 - val_sparse_categorical_accuracy: 0.5270 - learning_rate: 1.0000e-06\nEpoch 115/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9250 - sparse_categorical_accuracy: 0.5427\nEpoch 115: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 992ms/step - loss: 0.9250 - sparse_categorical_accuracy: 0.5427 - val_loss: 1.0037 - val_sparse_categorical_accuracy: 0.5269 - learning_rate: 1.0000e-06\nEpoch 116/1000\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - loss: 0.9269 - sparse_categorical_accuracy: 0.5416\nEpoch 116: sparse_categorical_accuracy did not improve from 0.54263\n\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 993ms/step - loss: 0.9269 - sparse_categorical_accuracy: 0.5416 - val_loss: 1.0044 - val_sparse_categorical_accuracy: 0.5268 - learning_rate: 1.0000e-06\nEpoch 116: early stopping\nRestoring model weights from the end of the best epoch: 106.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def top_p_logits_batch(logits: tf.Tensor, top_p: float = 1.0) -> tf.Tensor:\n    \"\"\"\n    Apply top-p (nucleus) filtering to a batch of logits.\n    Args:\n        logits: [batch, vocab]\n        top_p: cumulative probability threshold\n    Returns:\n        [batch, vocab]: Filtered logits.\n    \"\"\"\n    if top_p >= 1.0:\n        return logits\n\n    new_logits = []\n    for logit in logits:  # loop over batch\n        sorted_indices = tf.argsort(logit, direction='DESCENDING')\n        sorted_logits = tf.gather(logit, sorted_indices)\n        sorted_probs = tf.nn.softmax(sorted_logits)\n        cumulative_probs = tf.cumsum(sorted_probs)\n\n        # Mask tokens outside top_p\n        mask = cumulative_probs > top_p\n        mask = tf.concat([[False], mask[:-1]], axis=0)  # keep first above top_p\n        # Set masked logits to -inf\n        sorted_logits = tf.where(mask, tf.fill(tf.shape(sorted_logits), float('-inf')), sorted_logits)\n        # Scatter back to original order\n        new_logit = tf.scatter_nd(\n            indices=tf.expand_dims(sorted_indices, axis=-1),\n            updates=sorted_logits,\n            shape=tf.shape(logit, out_type=tf.int32)\n        )\n        new_logits.append(new_logit)\n\n    return tf.stack(new_logits, axis=0)\n\n\ndef sample_from_logits(logits: tf.Tensor, temperature: float = 1.0, top_k: int = 0, top_p: float = 1.0) -> tf.Tensor:\n    \"\"\"\n    Sample token IDs from logits with temperature scaling, top-k, and top-p filtering.\n    Args:\n        logits: [batch, vocab]\n        temperature: scaling factor\n        top_k: number of top tokens to consider\n        top_p: cumulative probability threshold\n    Returns:\n        [batch]: sampled token IDs\n    \"\"\"\n\n\n    # Scale by temperature\n    logits = logits / temperature\n\n    # Top-k filtering\n    if top_k > 0:\n        values, _ = tf.math.top_k(logits, k=top_k)\n        min_values = values[:, -1, tf.newaxis]\n        logits = tf.where(logits < min_values, tf.constant(-np.inf, dtype=logits.dtype), logits)\n                          \n\n    # Top-p\n    if top_p < 1.0:\n       logits = top_p_logits_batch(logits, top_p)\n    # Debug: print logits statistics\n    #print(\"sample_from_logits:\")\n    #print(\"  logits min/max:\", tf.reduce_min(logits).numpy(), tf.reduce_max(logits).numpy())\n    #print(\"  logits mean:\", tf.reduce_mean(logits).numpy())\n\n    # Convert to probabilities safely and sample\n    sampled_ids = tf.random.categorical(logits, num_samples=1)[:, 0]\n    return sampled_ids\n\n\ndef generate_drug_batch(\n    seed_texts,\n    model,\n    tokenizer,\n    max_length,\n    next_words=30,\n    temperature=1.0,\n    top_k=0,\n    top_p=1.0,\n    end_token_id=None,\n    min_length_before_eos=20,\n):\n    \"\"\"\n    Generate sequences from a model with autoregressive decoding.\n\n    Args:\n        seed_texts (list[str]): Starting strings for each sequence in the batch.\n        model (tf.keras.Model): Trained model for generation.\n        tokenizer: Tokenizer with `texts_to_sequences` & `index_word`.\n        max_length (int): Maximum sequence length.\n        next_words (int): Maximum number of tokens to generate.\n        temperature (float): Softmax temperature scaling.\n        top_k (int): Top-k sampling cutoff.\n        top_p (float): Nucleus (top-p) sampling cutoff.\n        end_token_id (int): ID of EOS token.\n        min_length_before_eos (int): Minimum tokens before EOS allowed.\n\n    Returns:\n        list[str]: Generated sequences.\n    \"\"\"\n    assert next_words <= max_length, \"next_words must be <= max_length\"\n\n    batch_size = len(seed_texts)\n\n    # Convert seeds to padded token sequences\n    token_lists = tokenizer.texts_to_sequences(seed_texts)\n    token_lists = pad_sequences(token_lists, maxlen=max_length, padding=\"post\")\n\n    finished = [False] * batch_size\n    step = 0\n\n    while step < next_words:\n        # Forward pass -> logits for next token\n        predicted_logits = model.predict(token_lists, verbose=0)  # [B, T, V]\n        logits = predicted_logits[:, -1, :]  # take last step logits [B, V]\n\n        # Mask out banned tokens\n        banned_tokens = [\n            tokenizer.word_index.get(\"<UNK>\"),\n            tokenizer.word_index.get(\"<start>\"),\n        ]\n        if end_token_id is not None and step < min_length_before_eos:\n            banned_tokens.append(end_token_id)\n\n        banned_tokens = [t for t in banned_tokens if t is not None]\n\n        if banned_tokens:\n            vocab_size = logits.shape[-1]\n            mask = tf.zeros(vocab_size, dtype=logits.dtype)\n            updates = tf.constant([-float(\"inf\")] * len(banned_tokens), dtype=logits.dtype)\n            mask = tf.tensor_scatter_nd_update(\n                mask,\n                indices=[[tid] for tid in banned_tokens],\n                updates=updates,\n            )\n            logits = logits + mask  # broadcast to [B, V]\n\n      \n        sampled_ids = sample_from_logits(logits, temperature, top_k, top_p)\n\n        for i, token_id in enumerate(sampled_ids.numpy()):\n            if finished[i]:\n                continue\n\n            if end_token_id is not None and token_id == end_token_id:\n                finished[i] = True\n                continue\n\n            word = tokenizer.index_word.get(token_id, None)\n            if word and word not in (\"<UNK>\", \"<start>\"):\n                seed_texts[i] += \" \" + word\n\n        # Roll sequence window forward\n        token_lists = tf.concat(\n            [token_lists[:, 1:], tf.expand_dims(sampled_ids, axis=-1)], axis=1\n        )\n\n        if all(finished):\n            break\n\n        step += 1\n\n    return seed_texts\n","metadata":{"execution":{"iopub.status.busy":"2025-08-29T15:51:14.172578Z","iopub.execute_input":"2025-08-29T15:51:14.172834Z","iopub.status.idle":"2025-08-29T15:51:14.186320Z","shell.execute_reply.started":"2025-08-29T15:51:14.172813Z","shell.execute_reply":"2025-08-29T15:51:14.185775Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Define prompts for generation\nprompts = [\"<start>\" for i in range(1000)]","metadata":{"execution":{"iopub.status.busy":"2025-08-29T15:51:14.187040Z","iopub.execute_input":"2025-08-29T15:51:14.187280Z","iopub.status.idle":"2025-08-29T15:51:14.211210Z","shell.execute_reply.started":"2025-08-29T15:51:14.187239Z","shell.execute_reply":"2025-08-29T15:51:14.210634Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Generate drug candidates\ngenerated = generate_drug_batch(\n    prompts,\n    model,\n    tokenizer,\n    max_length=50,\n    next_words=50,\n    temperature=1,\n    top_k=30,\n    top_p=0.95,\n    end_token_id=end_token_id\n)","metadata":{"execution":{"iopub.status.busy":"2025-08-29T15:51:14.211937Z","iopub.execute_input":"2025-08-29T15:51:14.212128Z","iopub.status.idle":"2025-08-29T15:52:33.074922Z","shell.execute_reply.started":"2025-08-29T15:51:14.212113Z","shell.execute_reply":"2025-08-29T15:52:33.074335Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"train_dataset = []\nfor smile in train_df['smiles'].sample(1000):\n    train_dataset.append(smile)\nprint(len(train_dataset))","metadata":{"execution":{"iopub.status.busy":"2025-08-29T15:54:19.979532Z","iopub.execute_input":"2025-08-29T15:54:19.979800Z","iopub.status.idle":"2025-08-29T15:54:19.988164Z","shell.execute_reply.started":"2025-08-29T15:54:19.979779Z","shell.execute_reply":"2025-08-29T15:54:19.987451Z"},"trusted":true},"outputs":[{"name":"stdout","text":"1000\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"\ngen_smiles =[]\nfor g in generated:\n    g = g.replace(' ','')\n    g = g.replace('<start>','')\n    try:\n        sm = sf.decoder(g)\n        gen_smiles.append(sm)\n    except Exception as e:\n        print(g,e)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-29T15:54:20.525181Z","iopub.execute_input":"2025-08-29T15:54:20.525609Z","iopub.status.idle":"2025-08-29T15:54:20.814361Z","shell.execute_reply.started":"2025-08-29T15:54:20.525588Z","shell.execute_reply":"2025-08-29T15:54:20.813804Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"! pip install rdkit-pypi scikit-learn fcd-torch -q","metadata":{"execution":{"iopub.status.busy":"2025-08-29T15:54:22.363489Z","iopub.execute_input":"2025-08-29T15:54:22.363734Z","iopub.status.idle":"2025-08-29T15:54:25.584576Z","shell.execute_reply.started":"2025-08-29T15:54:22.363718Z","shell.execute_reply":"2025-08-29T15:54:25.583390Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from rdkit import Chem, DataStructs\nfrom rdkit.Chem import AllChem, Descriptors, BRICS\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\n\n\n\nclass MoleculeEvaluator:\n    def __init__(self, gen_smiles, ref_smiles, radius=2, n_bits=2048):\n        self.gen_smiles = list(gen_smiles)  \n        self.ref_smiles = list(ref_smiles)\n        self.radius = radius\n        self.n_bits = n_bits\n        \n        self.gen_mols = [Chem.MolFromSmiles(s) for s in self.gen_smiles if Chem.MolFromSmiles(s)]\n        self.ref_mols = [Chem.MolFromSmiles(s) for s in self.ref_smiles if Chem.MolFromSmiles(s)]\n        \n        self.gen_fps = [AllChem.GetMorganFingerprintAsBitVect(m, self.radius, nBits=self.n_bits) for m in self.gen_mols]\n        self.ref_fps = [AllChem.GetMorganFingerprintAsBitVect(m, self.radius, nBits=self.n_bits) for m in self.ref_mols]\n\n    # Core Metrics\n\n    def internal_diversity(self):\n        \"\"\"Mean pairwise Tanimoto dissimilarity within generated molecules\"\"\"\n        n = len(self.gen_fps)\n        if n < 2:\n            return 0.0\n        dists = []\n        for i in range(n):\n            for j in range(i+1, n):\n                sim = DataStructs.TanimotoSimilarity(self.gen_fps[i], self.gen_fps[j])\n                dists.append(1 - sim)\n        return np.mean(dists)\n\n    def nearest_neighbor_similarity(self):\n        \"\"\"Similarity of each generated molecule to nearest neighbor in reference set\"\"\"\n        sims = []\n        for fp in self.gen_fps:\n            sim = max(DataStructs.BulkTanimotoSimilarity(fp, self.ref_fps))\n            sims.append(sim)\n        return np.mean(sims) if sims else 0.0\n\n    def scaffold_similarity(self):\n        \"\"\"Scaffold overlap between generated and reference sets\"\"\"\n        def get_scaffolds(mols):\n            scaffolds = set()\n            for m in mols:\n                scaff = MurckoScaffold.MurckoScaffoldSmiles(mol=m)\n                scaffolds.add(scaff)\n            return scaffolds\n\n        gen_scaff = get_scaffolds(self.gen_mols)\n        ref_scaff = get_scaffolds(self.ref_mols)\n        inter = len(gen_scaff.intersection(ref_scaff))\n        return inter / len(gen_scaff) if gen_scaff else 0.0\n\n    def fragment_similarity(self):\n        \"\"\"Fragment overlap (BRICS decomposition)\"\"\"\n        def get_fragments(mols):\n            frags = []\n            for m in mols:\n                parts = list(BRICS.BRICSDecompose(m))\n                frags.extend(parts)\n            return set(frags)\n\n        gen_frags = get_fragments(self.gen_mols)\n        ref_frags = get_fragments(self.ref_mols)\n        inter = len(gen_frags.intersection(ref_frags))\n        return inter / len(gen_frags) if gen_frags else 0.0\n\n    def novelty(self):\n        \"\"\"Fraction of generated molecules not in reference set\"\"\"\n        ref_set = set(self.ref_smiles)\n        novel = [s for s in self.gen_smiles if s not in ref_set]\n        return len(novel) / len(self.gen_smiles) if self.gen_smiles else 0.0\n\n    def validity(self):\n        \"\"\"Fraction of valid molecules\"\"\"\n        return len(self.gen_mols) / len(self.gen_smiles) if self.gen_smiles else 0.0\n\n    def unique_at_k(self, k=None):\n        \"\"\"Fraction of unique molecules (optionally top-k)\"\"\"\n        smiles = self.gen_smiles[:k] if k else self.gen_smiles\n        return len(set(smiles)) / len(smiles) if smiles else 0.0\n\n    def filters(self):\n        \"\"\"Simple Lipinski-like filter\"\"\"\n        passed = 0\n        for m in self.gen_mols:\n            mw = Descriptors.MolWt(m)\n            logp = Descriptors.MolLogP(m)\n            hbd = Descriptors.NumHDonors(m)\n            hba = Descriptors.NumHAcceptors(m)\n            if (mw < 500 and logp < 5 and hbd <= 5 and hba <= 10):\n                passed += 1\n        return passed / len(self.gen_mols) if self.gen_mols else 0.0\n\n    # Run all \n    def evaluate_all(self, unique_k=None):\n        return {\n            \"InternalDiversity\": self.internal_diversity(),\n            \"NearestNeighborSimilarity\": self.nearest_neighbor_similarity(),\n            \"ScaffoldSimilarity\": self.scaffold_similarity(),\n            \"FragmentSimilarity\": self.fragment_similarity(),\n            \"Novelty\": self.novelty(),\n            \"Validity\": self.validity(),\n            \"Unique@k\": self.unique_at_k(unique_k),\n            \"FiltersPass\": self.filters()\n        }\n\n\n\nmol_eval = MoleculeEvaluator(gen_smiles=gen_smiles,ref_smiles=train_dataset)","metadata":{"execution":{"iopub.status.busy":"2025-08-29T16:14:32.999696Z","iopub.execute_input":"2025-08-29T16:14:32.999991Z","iopub.status.idle":"2025-08-29T16:14:33.528730Z","shell.execute_reply.started":"2025-08-29T16:14:32.999972Z","shell.execute_reply":"2025-08-29T16:14:33.528136Z"},"trusted":true},"outputs":[{"name":"stderr","text":"[16:14:33] Explicit valence for atom # 5 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 6 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 11 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 5 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 7 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 1 K, 3, is greater than permitted\n[16:14:33] Explicit valence for atom # 12 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 3 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 4 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 3 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 4 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 1 K, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 3 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 4 K, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 6 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 2 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 3 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 4 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 4 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 5 Li, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 11 Cl, 3, is greater than permitted\n[16:14:33] Explicit valence for atom # 4 K, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 5 K, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 1 K, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 4 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 1 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 8 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 9 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 3 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 2 K, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 4 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 2 Na, 3, is greater than permitted\n[16:14:33] Explicit valence for atom # 1 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 7 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 9 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 3 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 7 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 7 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 3 Na, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 9 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 1 K, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 5 Cl, 2, is greater than permitted\n[16:14:33] Explicit valence for atom # 3 Cl, 2, is greater than permitted\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"mol_eval.evaluate_all()","metadata":{"execution":{"iopub.status.busy":"2025-08-29T16:14:33.530082Z","iopub.execute_input":"2025-08-29T16:14:33.530309Z","iopub.status.idle":"2025-08-29T16:15:11.190078Z","shell.execute_reply.started":"2025-08-29T16:14:33.530292Z","shell.execute_reply":"2025-08-29T16:15:11.189327Z"},"trusted":true},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"{'InternalDiversity': 0.8870384378472012,\n 'NearestNeighborSimilarity': 0.10365603236436531,\n 'ScaffoldSimilarity': 0.022222222222222223,\n 'FragmentSimilarity': 0.03024193548387097,\n 'Novelty': 1.0,\n 'Validity': 0.957,\n 'Unique@k': 0.999,\n 'FiltersPass': 0.9811912225705329}"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}